# WebAgent-R1
[WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)


## WeArena-Lite Benchmark

Comparison between existing methods and our WebAgent-R1 on the WebArena-Lite benchmark. The proposed method outperforms both strong prompting-based and finetuned baselines, achieving superior performance across various model sizes.

<p align="center">
  <img width="853" alt="benchmark" src="https://github.com/user-attachments/assets/67fe3b90-b035-44c8-8081-4417c635cf6c" />
</p>

## WebAgent-R1

- Top: Overview of the end-to-end multi-turn RL training framework used in WebAgent-R1.
- Bottom: An input/output example of agentâ€“web interaction at the k-th step. The interaction continues until either the maximum number of steps is reached or the agent generates an `exit()` action to signal task completion.

<p align="center">
  <img width="739" alt="overview" src="https://github.com/user-attachments/assets/18e7f495-385b-4ae8-8bd2-71ac74883a11" />
</p>

## Training Dynamics
Training dynamics during RL, including rewards, trajectory length, and number of interactions. As indicated by the dashed vertical lines in the figure, the entire process can be broadly divided into three phases: (1) initial skill acquisition, (2) exploration for policy refinement, and (3) final policy stabilization.


<p align="center">
  <img width="901" alt="image" src="https://github.com/user-attachments/assets/2a348153-5691-4134-9382-0a3803fe95c2" />
</p>

## Ablation study on RL initialization policy
We compared WebAgent-R1 (R1) with two variants:
- WebAgent-R1-Zero (R1-Zero), initialized from an off-the-shelf model without SFT
- WebAgent-R1-CoT (R1-CoT), initialized from an SFT model trained with long chain-of-thought (CoT) data during behavior cloning.

We report task success rate, single-turn response length, and number of interactions, evaluated both before and after applying RL.

<p align="center">
  <img width="737" alt="image" src="https://github.com/user-attachments/assets/658a76fd-e138-481b-92e3-c6c02dd486cb" />
</p>


## Analysis

### Promoting w/ Thinking vs. w/ Non-Thinking Format

Analysis of prompting design. We report the average success rate (SR), single-turn response length, and number of interactions. The result reveals a novel test-time scaling paradigm by increasing the number of interactions for multi-turn interactive web tasks.

<p align="center">
  <img width="445" alt="image" src="https://github.com/user-attachments/assets/36637aa9-1009-4feb-b5e5-64336c1a654f" />
</p>



### Test-time scaling through increased interactions

Analysis of test-time scaling with increased max number of interactions. Allowing more interactions enables the web agent to produce longer trajectories and consistently improves the success rate.

<p align="center">
  <img width="321" alt="image" src="https://github.com/user-attachments/assets/e6ccd1a1-1ff5-45d7-9e2c-b918d4c25192" />
</p>


## Case Study

### Model Output
Comparison of model outputs from WebAgent-R1 and WebAgent-R1-CoT. We present successful trajectories from both models on the same task (`What are the top-3 best-selling products in Jan 2023?`), showing only the first two steps for clarity (the entire trajectory is shown in the subsequent web screenshots for full context). 

Compared to WebAgent-R1, the long-CoT variant WebAgent-R1-CoT exhibits a more detailed thinking process.

<p align="center">
  <img width="730" alt="image" src="https://github.com/user-attachments/assets/ce4d8793-038d-471f-86a0-3d3d21d7fbd2" />
</p>


### Real-world Example

A real-world example of a successful trajectory generated by WebAgent-R1 on the task: `What are the top-3 best-selling products in Jan 2023?`

<p align="center">
  <img width="903" alt="image" src="https://github.com/user-attachments/assets/a49d6814-1f67-45c2-9e16-1ed8edc68b31" />
</p>


